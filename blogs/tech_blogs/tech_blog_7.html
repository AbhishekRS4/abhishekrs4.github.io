<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta name="google-site-verification" content="XXjK99sZKdpFN8kxwbglvDK8Gbhanfx-QfZGGG_TU1M" />
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 34px;
    }

    #paper_img{
      width: 200px;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>

  <title>Abhishek Ramanathapura Satyanarayana</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href="../../assets/css/tech_blog.css" rel="stylesheet">
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <heading>
                Tech Blog: Deploying an AWS Lambda function in Python with Kinesis streaming for a machine learning model
              </heading>
              <br>
              <span> Posted by <strong>Abhishek R. S.</strong> on <strong>2024-07-23</strong></span>

              <!-- General info about the project -->
              <h3>The blog is about...</h3>
              <p>
                Creating a serverless deployment using AWS Lamdba function in Python with Kinesis streaming for serving a machine learning model. 
                The project can be found in the following Github repo
                <a href="https://github.com/AbhishekRS4/ML_NY_Taxi/aws_lambda_kinesis"> ML_NY_TAXI/aws_lambda_kinesis </a>.
              </p>
              <p>
                Some of the benefits of using data streams is that it can support one to many relationship i.e., one publisher to many subscribers.
                Another important benefit is the retention period of data on streams is much larger and can be utilized for certain applications where
                http post request form of web service cannot be used because of lower request timeout.
              </p>

              <!-- Some specific info about the project -->
              <h3>1.1) IAM setup </h3>
              <p>
                Although, this is out of the scope of this blogpost, it is a standard practice to setup IAM users, groups and roles.
                Create an AWS user if not present already. Create a group for AWS Lambda users, Kinesis users, API Gateway admin, ECR users. 
                Add user to these groups. Add apporpriate policies to these groups by giving full access to relevant things.
                Create a role for lambda kinesis and add the policy for lambda kinesis execution access.
              </p>

              <h3>1.2) Setting up a Kinesis stream for ride and prediction events</h3>
              <p>
                A Kinesis stream can be created in AWS Kinesis. Refer to the following steps
                <br>
                Kinesis -> Kinesis Data Streams -> Create data stream -> Choose a name for the data stream -> Select provisioned for capacity mode ->
                Select 1 for provisioned shards -> Create data stream
                <br>
                Voila! A Kinesis data stream is ready to be utilized as a trigger for a Lambda function.
                Similarly, create another data stream for prediction where the predictions from the Lambda function will be published.
              </p>

              <h3>1.3) Creating a docker image for the lambda function</h3>
              <p>
                A list of publicly available docker images for AWS can be found in 
                <a href="https://gallery.ecr.aws/">Amazon Elastic Container Registry (ECR) public gallery</a>.
                One can search for "lambda python" and find a docker image for appropriate python version under <strong>Image tags</strong>.
                This image can be used as a base docker image to build a custom docker image for a lambda function.
              </p>

              <p>
                Unlike a typical <code>Dockerfile</code>, there will be certain changes.
                <ul>
                  <li>
                    <p>
                      There is no need to set working directory using <code>WORKDIR /app</code>
                      that is usually done in case of dockerizing a Flask / FastAPI application. 
                      There is a default working directory that is configured in the base docker image.
                    </p>
                  </li>
                  <li>
                    <p>
                      There is no need to expose a port for dockerizing the lambda function, 
                      that is usually done in case of dockerizing a Flask / FastAPI application.
                    </p>
                  </li>
                  <li>
                    <p>
                      The final <code>CMD</code> to run an application inside the container will just be the 
                      <br>
                      <code>lambda_function_script:lambda_function_handler</code>
                    </p>
                  </li>
                </ul>
              </p>

              <p>
                To build the docker container, use the following command,
                <br>
                <code>docker build -t aws-lambda-kinesis-ny-taxi .</code>
              </p>

              <h3>1.4) Creating a repo for docker container on AWS ECR</h3>
              <p>
                Create a private repo for the docker container using the following steps.
                ECR -> Create repo -> Select private -> Choose a name for the repo -> Create repo
              </p>
              <p>
                Once the repo is created, go to the repo on AWS, see <code>View Push Commands</code>. Run all the commands in the terminal.
              </p>

              <h3>1.5) Setting up the Lambda function on AWS with additional configurations</h3>
              <p>
                The AWS Lambda function needs to be set up. This can be done using the following steps.
                Lambda -> Create function -> Select from container image -> Choose a name for the lambda function -> 
                Choose the container image with browse images -> Choose x86_64 -> Choose existing role and select the role that was created earlier 
                ->  Create
              </p>

              <p>
                Add env variables to the lambda function. This can be done in config -> environment variables -> edit
              </p>

              <p>
                Add a trigger for the lambda function by selecting the appropriate data stream that was created earlier.
              </p>

              <p>
                Set a higher timeout like 30 sec. (for initial loading) and a slightly higher memory in Configuration -> General -> Timeout.
                After applying the above changes and test it, things will work. The first time will take a longer time to run (like > 20 sec.) and 
                the subsequent runs will be much faster (in milli sec.). The following image shows the successful test of the lambda function.
              </p>

              <h3>1.6) Attaching additional policies for the Lambda function role</h3>
              <p>
                Add an additional policy to the role that was created earlier to allow lambda function access to the S3 bucket, to load the saved model.
                This can be done using the following steps
                <br>
                Go to IAM -> Roles -> Select the role that was created earlier for the lambda kinesis execution -> Add permission -> Create inline policy
                -> Choose service (S3) -> Select all list and read actions -> In Resources specify ARN for bucket (bucket_name) and object (*) 
                Add ARN -> Next -> Select policy name -> Attach policy
              </p>

              <p>
                Add an additional policy to the role that was created earlier to allow lambda function to publish predictions to the prediction data stream.
                This can be done using the following steps
                <br>
                Go to IAM -> Roles -> Select the role that was created earlier for the lambda kinesis execution -> Add permission -> Create inline policy
                -> Choose service (Kinesis) -> Select putRecord and putRecords in write actions -> In Resources specify ARN for prediction stream 
                (stream_name and region) -> Add ARN -> Next -> Select policy name -> Attach policy
              </p>

              <h3>1.7) Verifying the working of the Lambda function in the terminal</h3>
              <p>
                In a terminal, run the following to send data to input Kinesis stream
                <br>
                <code>
                  KINESIS_STREAM_INPUT=ny-taxi-ride-event
                  <br>
                  <br>
                  aws kinesis put-record     --stream-name ${KINESIS_STREAM_INPUT}     --partition-key 1     --data '{
                    <br>
                    "ride": {
                      <br>
                        "PULocationID": 130,
                        <br>
                        "DOLocationID": 205,
                        <br>
                        "trip_distance": 3.66
                        <br>
                    }, 
                    <br>
                    "ride_id": 156
                    <br>
                  }'
                </code>
              </p>

              <p>
                In a terminal, run the following command to read data from the output Kinesis stream
                <br>
                <code>
                  KINESIS_STREAM_OUTPUT='ny-taxi-ride-duration-pred-event'
                  <br>
                  <br>
                  SHARD='shardId-000000000000'
                  <br>
                  <br>
                  SHARD_ITERATOR=$(aws kinesis \
                  <br>
                    get-shard-iterator \
                    <br>
                    --shard-id ${SHARD} \
                    <br>
                    --shard-iterator-type TRIM_HORIZON \
                    <br>
                    --stream-name ${KINESIS_STREAM_OUTPUT} \
                    <br>
                    --query 'ShardIterator' \
                    <br>
                  )
                  <br>
                  <br>
                  RESULT=$(aws kinesis get-records --shard-iterator $SHARD_ITERATOR)
                  <br>
                  <br>
                  echo ${RESULT} | jq -r '.Records[0].Data' | base64 --decode
                </code>
              </p>

              <p>
                The following image shows the success of the publishing data to the Kinesis input stream.
              </p>
              <div class="featured-image">
                <img src="../../assets/images/tech_blogs/blog_7/aws_lambda_kinesis_stream_input_success.png" width="100%">
              </div>

              <p>
                The following image shows the success of reading the data from the Kinesis output stream and decoding it.
              </p>
              <div class="featured-image">
                <img src="../../assets/images/tech_blogs/blog_7/aws_lambda_kinesis_stream_output_success.png" width="100%">
              </div>

              <h3>1.8) Monitoring CloudWatch Logs</h3>
              <p>
                One can easily look at the CloudWatch logs. This can be done from the lambda function console. 
                Go to Lambda function console -> Monitor -> View CloudWatch logs -> Select the log stream to view the log events
              </p>

              <h3>1.9) Challenges faced</h3>
              <ul>
                <li>
                  <p>
                    I faced some issues with package dependencies while building the docker container image. The main issue was with <code>boto3</code> package.
                    The package is already installed in the base docker image but the newer <code>MLFlow</code> version will require a newer <code>boto3</code> 
                    version. This is because <code>MLFlow</code> will use <code>boto3</code> SDK for AWS S3 bucket operations for handling model artifacts. 
                    However, I was able to resolve the issue with a quick update of <code>boto3</code>.
                  </p>
                </li>
                <li>
                  <p>
                    The other challenge was with minor typos in the policies attached to the role. The typos were in the policies - 
                    <ul>
                      <li>
                        <p>
                          The policy for giving the lambda function access to S3 bucket for loading the saved model. The issue was a typo in the bucket name.
                        </p>
                      </li>
                    </ul>
                    <ul>
                      <li>
                        <p>
                          The policy for giving the lambda function access to publish the prediction to the output Kinesis stream. The issue was a typo in 
                          the ARN for the stream.
                        </p>
                      </li>
                    </ul>
                    After fixing these issues, things worked smoothly.
                  </p>
                </li>
              </ul>

              <!-- Some conclusions, learnings during this project -->
              <h3>Main takeaway</h3>
              <p>
                <ul>
                  <li>
                    <p>
                      I learned to create a lambda function in Python with AWS Kinesis streaming for deploying a machine learning model. I learned to 
                      configure env variables for the lambda function.
                    </p>
                  </li> 
                  <li>
                    <p>
                      I learned to create Kinesis data streams. I learned to use one data stream to trigger the lambda function and another 
                      to publish the predictions of the machine learning model.
                    </p>
                  </li>
                  <li>
                    <p>
                      I learned to attach additional policies for the role to give the lambda function access to S3 bucket to load the saved model and 
                      access to publish the predictions to a Kinesis data stream . 
                    </p>
                  </li>
                </ul>
                I learned a lot of new things in this project. To deploying more complex ML models using AWS Lambda, Kinesis in the future &#128516;.
              </p>

              <!-- Future work ideas, to build on this project -->
              <h3>Next Steps</h3>
              <ul>
                <li>
                  <p>
                    Deploy multiple complex ML models to AWS Lambda with Kinesis streaming.
                  </p>
                </li>
                <li>
                  <p>
                    Deploy lambda functions with different trigger variants like adding a trigger to S3 bucket.
                  </p>
                </li>
              </ul>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <p>
            <i>
              <strong>About the author</strong>: Abhishek is a graduate AI engineer who is interested
              in developing and deploying Artificial Intelligence, Machine Learning, Computer Vision, Data Science
              applications. His goal is to improve human lives by leveraging real world data.
            </i>
          </p>
        </table>

      </td>
    </tr>
  </table>
</body>

</html>
